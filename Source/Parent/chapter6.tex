% Chapter Chapter 6 For Reproducible Research in R and RStudio
% Christopher Gandrud
% Created: 16/07/2012 05:45:03 pm CEST
% Updated: 27 December 2012




\chapter{Gathering Data with R}\label{DataGather}

How you gather your data directly impacts how reproducible your research will be. Of course you should try your best to document every step of your data gathering process. Reproduction will be easier if your documentation--especially, variable descriptions and source code--makes it easy for you and others to understand what you have done. If all of your data gathering steps are tied together by your source code, then independent researchers (and you) can more easily regather the data. Regathering data will be easiest if running your code allows you to get all the way back to the raw data--the rawer the better. Of course this may not always be possible. You may need to conduct interviews or compile information from paper based archives, for example. The best you can sometimes do is describe your data gathering process in detail. Nonetheless, R's automated data gathering capability for internet-based information is extensive. Learning how to take full advantage of these capabilities greatly increases reproducibility and can save you considerable time and effort over the long run.

In this chapter you'll learn how to gather quantitative data in a reproducible and, in some cases, fully replicable way. You'll start by learning how to use data gathering make files\index{make file} to organize your whole data gathering process so that it can be completely reproduced. Then you will learn the details of how to actually load data into R from various sources, both locally on your computer and via the internet. In the next chapter (Chapter \ref{DataClean}) you'll learn the details of how to clean up raw data so that it can be merged together into data frames that you can use for statistical analyses.

%%%%%%%%%%%%% Organizing data gathering
\section{Organize your data gathering: make files}

Before getting into the details of using R to gather data, let's start by creating a plan to organize the process. Organizing your data gathering process from the beginning of a research project improves the possibility of reproducibility and can save you significant effort over the course of the project by making it easier to add and regather data later on. 

A key part of reproducible data gathering with R, like reproducible research in general, is segmenting the process into discrete files that can all be run by a common ``make'' file\index{make file}. In this chapter we'll learn how to create make-like files run exclusively from R as well as GNU Make,\footnote{GNU stands for ``GNU's Not Unix", indicating that it is Unix-like.}\index{GNU make} which you run from a shell.\footnote{To standardize things, I use the terms ``R make-like file'' for files created and run in R and the standard ``makefile'' for files run by Make.} Learning how to create R make-like files is fairly easy. Using GNU Make does require learning yet more new syntax. However, it has one very clear advantage: it only runs a source code file that has been updated since the last time you called the makefile. This is very useful if part of your data gathering process is very computationally and time intensive.

Segmentation your data gathering and it together with some sort of make file allows you to more easily navigate research text and find errors in the source code. The make file's output is the data set(s) that you'll use in the statistical analyses. There are two types of source code files that the make file runs: data gathering/clean up files and merging files. Data clean up files bring raw individual data sources into R and transform them so that can be merged together with data from the other sources. Some of the R tools for data clean up and merging will be covered in Chapter \ref{DataClean}. In this chapter we mostly cover the ways to bring raw data into R. Merging files are executed by the make file after it runs the data gathering/clean up files.

It's a good idea to have the source code files use very raw data as input. Your source code should avoid directly changing these raw data files. Instead changes should be output to new objects and data files. Doing this makes it easier to reconstruct the steps you took to create your data set. Also, while cleaning and merging your data you may transform it in an unintended way, for example, accidentally deleting some observations that you had wanted to keep. Having the raw data makes it easy to go back and correct your mistakes. 

\subsection{R Make-like files}

When you create make-like files in R to organize and run your data gathing you usually only need one or two commands {\tt{setwd}}\index{setwd} and {\tt{source}}\index{source}. As we talked about in Chapter \ref{DirectoriesChapter}, {\tt{setwd}} simply tells R where to look for and place files. {\tt{source}} tells R to run code in an R source code file.\footnote{We use the {\tt{source}} command is used more in the Chapter \ref{StatsModel}.}  Lets see what a data make file might look like for our example project (see Figure \ref{ExampleTree}). The file paths in this example are for Unix-like systems.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{################}
\hlcomment{# Example R make-like file}
\hlcomment{# Christopher Gandrud}
\hlcomment{# Updated 15 January 2012}
\hlcomment{################}

\hlcomment{# Set working directory}
\hlfunctioncall{setwd}(\hlstring{"/ExampleProject/Data/"})

\hlcomment{# Gather and clean up raw data files.}
\hlfunctioncall{source}(\hlstring{"/GatherSource/IndvDataGather/Gather1.R"})

\hlfunctioncall{source}(\hlstring{"/GatherSource/IndvDataGather/Gather2.R"})

\hlfunctioncall{source}(\hlstring{"/GatherSource/IndvDataGather/Gather3.R"})
	
\hlcomment{# Merge cleaned data frames into data frame object CleanedData}
\hlfunctioncall{source}(\hlstring{"GatherSource/MergeData.R"})

\hlcomment{# Save cleaned & merged Data as MainData.csv}
\hlfunctioncall{write.csv}(CleandedData, file = \hlstring{"/DataFiles/MainData.csv"})
\end{alltt}
\end{kframe}
\end{knitrout}


This code first sets the working directory. Then it runs three source code files to gather data from three different sources. These files gather the data and clean it so that it can be merged together. The cleaned data frames are available in the current workspace. Next the make file runs a source code file that merges the data frames. Finally, it saves the output data frame {\emph{CleanData}} as a CSV\indxe{CSV} formated file called {\emph{MainData.csv}} using the {\tt{write.csv}}\index{write.csv} command. In our example project, this would be the main file we use for statistical analysis. 

You can run the commands in this file one by one or run the make-like through the \texttt{source} command to run it all at once.

\subsection{GNU Make}

R make-like files are a simple way to tie together a segmented data gathering process. If one or more of the source files that our example before runs is computationally intensive it is a good idea to run them only when they are updated. However, this can become tedious, especially if there are many segments. The well established GNU Make\index{GNU Make} command line program\footnote{GNU Make was originally developed in 1977 by Stuart Feldman as a way to compile computer programs from a series of files, its primary use to this day. For an overview see: \url{http://en.wikipedia.org/wiki/Make_(software)}.} deals with this problem by comparing the output files' time stamps\footnote{A file's time stamp records the time and date when it was last changed.}\index{time stamp} to time stamps of the source files that created them. If a source file has a time stamp that is newer than it's output, Make will run it. If the source's time stamp is older than it's output, Make will skip it.

In Make terminology the output files are called ``targets''\index{targets, Make} and the files that create them are called ``prerequisites''\index{prerequisites, Make}. You specify a ``recipe''\index{recipe, Make} to create the targets from the prerequisites. The general form is:

\begin{knitrout}
    \definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}
    \color{fgcolor}
    \begin{kframe}
        \begin{verbatim}
TARGET ... : PREREQUISITE ...
    RECIPIE
    ... 
    ...
            \end{verbatim}
        \end{kframe}
\end{knitrout}

Note that, unlike in R, tabs are important in Make. The indicate what lines are the recipe. Make uses the recipe to ensure that targets are newer than prerequisites. If they are newer, it does nothing.

If you are using a Linux or Mac computer you already have Make installed.\footnote{To verify this open the Terminal\index{Terminal} and type: \texttt{make --version}. This should output details about the current version of Make installed on your computer.} Windows users will have Make installed if they have already installed Rtools\index{Rtools} (see page \pageref{RtoolsDownload}).

The basic of reproducible data gathering with Make is similar to what we saw before, with a few twists and some new syntax. Let's see an example that does what we saw before: gather data from three sources, clean and merge the data and save it as in CSV\index{CSV} format. 

\paragraph{Example Makefile}

The first thing we need to do is create a new file called \emph{Makefile}\footnote{Alternatively you can call the file \emph{GNUmakefile} or \emph{makefile}.} and place it in the same directory as the data gathering files. Before modifying the makefile first make sure that the \texttt{write.csv} command is moved into the \emph{MergeData.R} file. Then in the makefile write:

\begin{knitrout}
	\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}
	\color{fgcolor}
	\begin{kframe}
		\begin{verbatim}
################
# Example Makefile
# Christopher Gandrud
# Updated 15 January 2012
# Influenced by Rob Hyndman (31 October 2012)
# See: http://robjhyndman.com/researchtips/makefiles/
################

# Key variables to define
RDIR = .

# Create list of R source files
RSOURCE = $(wildcard $(RDIR)/*.R)

# Files to indicate when the RSOURCE file was run
OUT_FILES = $(RSOURCE:.R=.Rout)

# Default target
all: $(OUT_FILES) 

# Run the RSOURCE files
$(RDIR)/%.Rout: $(RDIR)/%.R
    R CMD BATCH $<

# Remove Out Files
clean: 
    rm -fv $(OUT_FILES)
		\end{verbatim}
	\end{kframe}
\end{knitrout}

\noindent Ok, let's break down the code. The first part of the file defines variables that will be used later on. For example, in the first line of executable code (\texttt{RDIR = .}) we create a simple variable\footnote{Simple string variables are often referred to as ``macros'' in GNU Make. A common convention in Make and Unix-like shells generally is to use all caps for variable names.} called \texttt{ROUT} with a period (\texttt{.}) as its value. In Make, and Unix-like shells periods indicate the current directory. 

The second line (\verb|RSOURCE:= $(wildcard $(RDIR)/*.R)|) creates a variable containing a list of all of the names of files with the extension \emph{.R}, i.e. our data gathering and merge source code files. This line has some new syntax, so let's work through it. The dollar signs (\verb|$|). In make (and Unix-like shells generally) a dollar sign (\verb|$|) followed by a variable name substitutes the variable name for the value of the variable.\footnote{This is a kind of parameter expansion\index{parameter expansion}. For more information about parameter expansion see \citep{Frazier2008}.} For example, \verb|$(RDIR)| inserts the period \texttt{.} that we defined as the value of \texttt{RDIR} previously. The parentheses are included to clearly demarcate where the variable name begins and ends.\footnote{Braces (\texttt{\{\}}) are also sometimes used for this.} 

The asterisk (\verb|*|) is a ``wildcard''. A wildcard\index{wildcard} is a special character that allows you to select file names. The asterisk wildcard selects any file name. Using \verb|*.R| selects any file name that ends in \emph{.R}.  

Why did we include the actual word \texttt{wildcard}?\index{wildcard, Make function} The \texttt{wildcard} function is different from the asterisk wildcard character. The function creates a list of files that match a pattern. In this case the pattern is \verb|$(RDIR)/*.R|. The general form for writing the \texttt{wildcard} function is: \verb|$(wildcard PATTERN)|.

The third line (\verb|OUT_FILES = $(RSOURCE:.R=.Rout)|) creates a variable for the \emph{.Rout} files that Make will use to tell how recently each R file was run. \verb|$(RSOURCE:.R=.Rout)| is a variable that uses the same file name as our RSOURCE files, but uses the file extension \emph{.Rout}.

The second part of the make file tells Make what we want to create and how to create it. In the line (\verb|all: $(OUT_FILES|) we are specifying the makefile's default target.\index{target, makefile} Targets are the files that you instruct Make to make. \texttt{all:} sets the default target, it is what Make tries to create when you enter the command \textttt{make} in the terminal with now arguments. We will see later how to instruct Make to compile different targets.

The next two executable lines (\verb|$(RDIR)/%.Rout: $(RDIR)/%.R| and \verb|R CMD BATCH $<|) actually runs the R source code files in the directory.  The first line specifies that the \emph{.Rout} files are the targets of the \emph{.R} files (the prerequisites. The percent sign (\verb|%|) is another wildcard. Unlike the asterisk, it replaces the selected file names throughout the command used to create the target.

The dollar and less than signs (\verb|$<|) indicate the first prerequisite for the target, i.e. the \emph{.R} files. \texttt{R CMD BATCH}\index{R CMD BATCH} is a way to call R from a Unix-like shell, run source files and output the results to other files. In Windows you will probably need to change \texttt{R} to the path for your \emph{R.exe} file. For example: \emph{C:\textbackslash{}Programs Files\textbackslash{}R\textbackslash{}2.15.2\textbackslash{}bin\textbackslash{}R.exe}. The outfiles it creates have the extension \emph{.Rout}.

The last two lines specify another target: \texttt{clean}. When you type \texttt{make clean} into your shell Make will follow the recipe: \verb|rm -fv $(OUT_FILES)|.\index{rm} This removes (deletes) the \emph{.Rout} files. The \texttt{f} argument (force) ignores  files that don't exist and the \texttt{v} argument (verbose) tells you what is happening. When you delete the \emph{.Rout} files, Make will run all of the \emph{.R} files the next time you call it.

\paragraph{Running the MakeFile}

To run the makefile simply type \texttt{make} into your shell. It will create the CSV final data file and three files with the extension \emph{.Rout}, indicating when the segmented data gathering files were last run.\footnote{If you open these files you fill find the output from the R session used when the their source file was last run.} Remember to make sure your current working directory is the one with the files you want to run. 

When you run \verb|make| in the shell for the first time you should get the output:

\begin{knitrout}
	\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}
	\color{fgcolor}
	\begin{kframe}
		\begin{verbatim}
## R CMD BATCH Gather1.R
## R CMD BATCH Gather2.R
## R CMD BATCH Gather3.R
## R CMD BATCH MergeData.R
			\end{verbatim}
		\end{kframe}
\end{knitrout}

\noindent If you run it a second time without changing the R source files you will get the following output:

\begin{knitrout}
	\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}
	\color{fgcolor}
	\begin{kframe}
		\begin{verbatim}
## make: Nothing to be done for `all'.
			\end{verbatim}
		\end{kframe}
\end{knitrout}

\noindent To remove the \emph{.Rout} files use set the make target to \texttt{clean}:

\begin{knitrout}
	\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}
	\color{fgcolor}
	\begin{kframe}
		\begin{verbatim}
make clean

## rm -fv ./Gather1.Rout ./Gather2.Rout ./Gather3.Rout 
## ./MergeData.Rout
## ./Gather1.Rout
## ./Gather2.Rout
## ./Gather3.Rout
## ./MergeData.Rout
			\end{verbatim}
		\end{kframe}
\end{knitrout}


\paragraph{Other information about Makefiles}

Note that Make relies heavily on commands and syntax of the shell program that you are using. The above example was written and tested on a Mac. It should work on other Unix-like computers without modification.

You can use Make to build almost any project from the shell, not just run R source code files. It was an integral part of early reproducible computational research \citep{Fomel2009, Buckheit1995}. Rob Hyndman more recently posted a description of the makefile he uses to create a project with R and Latex.\footnote{See his blog at: \url{http://robjhyndman.com/researchtips/makefiles/}. Posted 31 October 2012. This method largely replicates what we do in this book with \emph{knitr}. Nonetheless, it has helpful information about Make that can be used in other tasks. It was in fact helpful for writing this section of the book.} The complete source of information on GNU Make is the online manual. It is available at: \url{http://www.gnu.org/software/make/manual/}.

\section{Importing locally stored data sets}

Now that we've covered the big picture, let's learn the different tools you will need to know to gather data from different types of sources. The most straightforward place to load data from is a local file, e.g. one stored on your computer. Though storing your data locally does not really encourage reproducibility, most research projects will involve loading data this way at some point. The tools you will learn for importing locally stored data files will also be important for most of the other methods further on. Let's briefly look at how to load single and multiple files locally.

\subsection{Importing a single locally stored file}

As we have seen, plain-text file based data stored on your computer can be loaded into R using the \texttt{read.table}\index{read.table} command. If you are using RStudio you can do the same thing with drop down menus. To open a plain-text data file click on \texttt{Workspace} \textrightarrow\: \texttt{Import Dataset\ldots} \textrightarrow\: \texttt{From Text File\ldots}. In the box that pops up, specify the separator, whether or not you want the first line to be treated as variable labels, and other options. This is initially easier than using \texttt{read.table}. But it is less reproducible.

If the data is not stored in plain-text format, but is instead saved by another statistical program such as SPSS, SAS, or Stata, we can import it using commands in the \emph{foreign} package\index{foreign}. For example, imagine we have a data file called \emph{Data1.dta} stored in our working directory. This file was created by the Stata\index{Stata} statistical program. To load the data into an R data frame object called \emph{StataData} simply type:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{# Load library}
\hlfunctioncall{library}(foreign)

\hlcomment{# Load Stata formatted data}
StataData <- \hlfunctioncall{read.dta}(file = \hlstring{"Data1.dta"})
\end{alltt}
\end{kframe}
\end{knitrout}


As you can see, commands in the \emph{foreign} library have similar syntax to \texttt{read.table}. To see the full range of commands and file formats that the \emph{foreign} package supports use the following command:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlfunctioncall{library}(help = \hlstring{"foreign"})
\end{alltt}
\end{kframe}
\end{knitrout}


If you have data stored in a spreadsheet format such as Excel's \emph{.xlsx}, it may be best to first clean up the data in the spreadsheet program by hand and then save the file in plain-text format. When you clean up the data make sure that the first row has the variable names and that observations are in the following rows. Also, remove any extraneous information--notes, colors, and so on--that will not be part of the data frame.

To aid reproducibility, locally stored data should include careful documentation of where the data came from and how, if at all, it was transformed before it was loaded into R. Ideally the documentation would be written in a text file saved in the same directory as the raw data file. 

\section{Importing data sets from the internet}

There are many ways to import data that is stored on the internet directly into R. We have to use different methods depending on where and how the data is stored. 

\subsection{Data from non-secure ({\tt{http}}) URLs}

Importing data into R that is located at a non-secure URL--ones that start with {\tt{http}}--is straightforward provided that:

\begin{itemize}
	\item the data is stored in a simple format, e.g. plain-text,
	\item the file is not embedded in a larger HTML website.
\end{itemize}

We have discussed the first issue in detail. You can determine if the data file is embedded in a website by opening the URL. If you only see the raw plain-text data, you are probably good to go.\footnote{If the data is embedded in a larger website--the way data is usually stored on the cloud version of Dropbox--you may still be able to download it into R. However, this can be difficult and varies depending on the structure of the website. So, I do not cover it in this book.}

To import the data simply include the URL as the file name in your \texttt{read.table} command. We saw earlier how to download a CSV data file from a Dropbox \emph{Public} folder with the shortened URL \texttt{http://bit.ly/PhjaPM}:

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
Data <- \hlfunctioncall{read.table}(\hlstring{"http://bit.ly/PhjaPM"}, 
					sep = \hlstring{","}, header = TRUE)
\end{alltt}
\end{kframe}
\end{knitrout}


\subsection{Data from secure ({\tt{https}}) URLs}\label{SecureDataDownload}

\noindent We have to take a few extra steps to download data from a secure URL. You can tell if the data is stored at a secure web address if it begins with \texttt{https} rather than \texttt{http}. We need the help of the \texttt{getURL}\index{getURL} command in the {\emph{RCurl}} package \cite[]{R-RCurl} and \texttt{textConnection}. The latter command is in base R. The two rules about data being stored in plain text-formats and not being embedded in a large HTML website apply to secure web addresses as well.

Let's try an example. I have data in comma-separated values format stored at a GitHub\index{GitHub} repository. The URL for the ``raw" (plain-text) version of the data is \url{https://raw.github.com/christophergandrud/Disproportionality_Data/master/Disproportionality.csv}.\footnote{To find the URL for the raw version of a file on the GitHub website simply click the \texttt{Raw} button on the right just above the file preview.} Imagine that we put the address as a character string into an object called \emph{UrlAddress} (not shown).\footnote{See page \pageref{Objects} for how to put a character string into an object. I do not show how this was done in the book, due to space constraints.}
To download it into R we could use this code:




{\small
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{# Load package}
\hlfunctioncall{library}(RCurl)

\hlcomment{# Pull data from the internet}
DataUrl <- \hlfunctioncall{getURL}(UrlAddress)

\hlcomment{# Convert Data into a data frame}
Data <- \hlfunctioncall{read.table}(\hlfunctioncall{textConnection}(DataUrl), 
					sep = \hlstring{","}, header = TRUE)
					
\hlcomment{# Show variables in data}
\hlfunctioncall{names}(Data)
\end{alltt}
\begin{verbatim}
## [1] "country"            "year"               "disproportionality"
\end{verbatim}
\end{kframe}
\end{knitrout}

}

\subsection{Compressed data stored online}

Sometimes data files are large, making them difficult to store and download without compressing\index{file compression} them. There are a number of compression methods such as Zip and tar.\footnote{Tar archives are sometimes referred to as `tar balls'.} Zip files have the extension {\tt{.zip}} and tar files use extensions such as {\tt{.tar}} and {\tt{.gz}}. In most cases\footnote{Some formats that require the {\emph{foreign}} package to open are more difficult. This is because functions such as {\tt{read.dta}} for opening Stata {\tt{.dta}} files only accept file names or URLs as arguments, not connections, which you create for unzipped files.} you can download, decompress, and create data frame objects from these files directly in R. 

To do this you need to:\footnote{The description of this process is based on a Stack Overflow comment by Dirk Eddelbuettel (see {\url{http://stackoverflow.com/questions/3053833/using-r-to-download-zipped-data-file-extract-and-import-data?answertab=votes\#tab-top}}, accessed 16 July 2012.)}

\begin{itemize}
	\item create a temporary file with {\tt{tempfile}} to store the zipped file, which you will later remove with the {\tt{unlink command}} at the end,
	\item download the file with {\tt{download.file}},
	\item decompress the file with one of the {\tt{connections}} commands in base R,\footnote{To find a full list of commands type {\tt{?connections}} into the R console.}
	\item read the file with {\tt{read.table}}. 
\end{itemize}

\noindent The reason that we have to go through so many extra steps is that compressed files are more than just a single file, but can contain a number of files as well as metadata.

Let's download a compressed file called {\emph{uds\_summary.csv}} from \cite{Pemstein2010}. It is in a compressed file called {\emph{uds\_summary.csv.gz}}. The file's URL address is {\url{http://www.unified-democracy-scores.org/files/uds_summary.csv.gz}}, that I shortened\footnote{Again, I used bitly (\url{bitly.com}) to shorten the URL.} to \url{http://bit.ly/S0vxk2} because of space constraints.

{\footnotesize
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{# For simplicity, store the URL in an object called \hlstring{'url'}.}
url <- \hlstring{"http://bit.ly/S0vxk2"}

\hlcomment{# Create a temporary file called \hlstring{'temp'} to put the zip file into.}
temp <- \hlfunctioncall{tempfile}()

\hlcomment{# Download the compressed file into the temporary file.}
\hlfunctioncall{download.file}(url, temp)

\hlcomment{# Decompress the file and convert it into a dataframe}
\hlcomment{# class object called \hlstring{'data'}.}
Data <- \hlfunctioncall{read.csv}(\hlfunctioncall{gzfile}(temp, \hlstring{"uds_summary.csv"}))

\hlcomment{# Delete the temporary file.}
\hlfunctioncall{unlink}(temp)

\hlcomment{# Show variables in data}
\hlfunctioncall{names}(Data)
\end{alltt}
\begin{verbatim}
## [1] "country" "year"    "cowcode" "mean"    "sd"      "median"  "pct025" 
## [8] "pct975"
\end{verbatim}
\end{kframe}
\end{knitrout}

}

\subsection{Data APIs \& feeds}

There are a growing number of packages that can gather data directly from a variety of internet sources and import them into R. Most of these packages use the sources' application programing interface (API)\index{API} that allow programs interact with the website. Needless to say, this is great for reproducible research. It not only makes the data gathering process easier as you don't have to download many Excel files and fiddle around with them before even getting the data into R, but it also makes replicating the data gathering process much more straightforward. Some examples of these packages include: 

\begin{itemize}
	\item The \emph{openair} package \citep{R-openair}, which beyond providing a number of tools for analyzing air quality data also has the ability to directly gather data directly from sources such as Kings College London's London Air (\url{http://www.londonair.org.uk/}) database with the \texttt{importKCL} command.
	\item The \emph{quantmod} package \citep{R-quantmod} allows you to access data from Google Finance,\footnote{\url{http://www.google.com/finance}} Yahoo Finance\footnote{\url{http://finance.yahoo.com/}} and the US Federal Reserve's FRED\footnote{\url{http://research.stlouisfed.org/fred2/}} economic database.
	\item The \emph{treebase} package by \cite{Boettiger2012} allows you to access phylogenetic data from TreeBASE.\footnote{\url{http://treebase.org}}
	\item The \emph{twitteR} package \citep{R-twitteR} access Twitter's\footnote{\url{https://twitter.com/}} API. This allows you to download data from twitter including tweets and trending topics.
	\item The \emph{WDI} package \citep{R-WDI} allows you to directly download data from the World Bank's Development Indicators database.\footnote{\url{http://data.worldbank.org/data-catalog/world-development-indicators}} This database includes numerous country-level economic, health, and environment variables. 
\end{itemize}

The rOpenSci\footnote{\url{http://ropensci.org/}}\index{rOpenSci} group has and is developing a number of packages for accessing scientific data from web-based sources with R. They have a comprehensive set of packages for accessing biological data and academic journals. For a list of their packages see: \url{http://ropensci.org/packages/index.html}. Another fairly comprehensive and regularly updated list of APIs available as R package on Stack Exchange's Cross Validated website.\footnote{{\small{\url{http://stats.stackexchange.com/questions/12670/data-apis- feeds-available-as-packages-in-r}}}}

\paragraph{API Package Example: World Bank Development Indicators}

Each of these packages has its own syntax and it is impossible to go over all of them here. Nonetheless, let's look at an example of accessing World Bank data with the \emph{WDI}\index{WDI} to give you a sense of how these packages work. Imagine that we want to gather data on countries' methane emissions. We can use \emph{WDI}'s \texttt{WDIsearch} command to find methane emissions data available at the World Bank:

{\scriptsize
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{# Load WDI package}
\hlfunctioncall{library}(WDI)

\hlcomment{# Search World Bank for methane data}
\hlfunctioncall{WDIsearch}(\hlstring{"methane emissions"})
\end{alltt}
\begin{verbatim}
##      indicator             
## [1,] "EN.ATM.METH.AG.KT.CE"
## [2,] "EN.ATM.METH.AG.ZS"   
## [3,] "EN.ATM.METH.EG.KT.CE"
## [4,] "EN.ATM.METH.EG.ZS"   
## [5,] "EN.ATM.METH.IN.ZS"   
## [6,] "EN.ATM.METH.KT.CE"   
##      name                                                                         
## [1,] "Agricultural methane emissions (thousand metric tons of CO2 equivalent)"    
## [2,] "Agricultural methane emissions (% of total)"                                
## [3,] "Methane emissions in energy sector (thousand metric tons of CO2 equivalent)"
## [4,] "Energy related methane emissions (% of total)"                              
## [5,] "Industrial methane emissions (% of total)"                                  
## [6,] "Methane emissions (kt of CO2 equivalent)"
\end{verbatim}
\end{kframe}
\end{knitrout}

}

\noindent This shows us a selection of indicator numbers and their name.\footnote{You can also search the website itself. The indicator numbers are at the end of each indicators' URL.} Let's gather data on countries' agricultural methane emissions as a percentage of total methane emissions. The indicator number for this variable is EN.ATM.METH.AG.ZS. Now we can use the command \texttt{WDI} to gather the data and put it in an object called \emph{AgMethane}.

\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}\begin{kframe}
\begin{alltt}
\hlcomment{# Gather agricultural methane emissions date from WDI}
AgMethane <- \hlfunctioncall{WDI}(indicator = \hlstring{"EN.ATM.METH.AG.ZS"})
\end{alltt}


{\ttfamily\noindent\textcolor{warningcolor}{\#\# Warning: Unable to download indicators  EN.ATM.METH.AG.ZS}}\begin{alltt}

\hlcomment{# Show head of AgMethane data frame}
\hlfunctioncall{head}(AgMethane)
\end{alltt}
\begin{verbatim}
## NULL
\end{verbatim}
\end{kframe}
\end{knitrout}


\section{Basic web scraping}

If a package does not already exist to access data from a particular website there are other ways to automatically ``scrape'' data from the website with R.\index{web scraping} This section gives a very basic introduction to some of these techniques to get you headed in the right direction.

\subsection{Accessing APIs with httr}

If a website has an API, but there is R package available yet to access it you can probably still get at the data with the \emph{httr} package \citep{R-httr}. To get a sense of how to download data from an API with \emph{httr} let's look at a simple example of 

\subsection{Gathering and parsing text from the web}

If no API is available to access the data you want from a website you can try to scrape the website. The most basic way to do this is with the \emph{RCurl} package \citep{R-RCurl}. Virtually all of the tools for getting data from the a URL discussed so far use \emph{RCurl}. The \emph{httr} package is simply an \emph{RCurl} package that makes accessing APIs easier. The package allows you to transfer files from a URL into R.  

\subsection{Scraping tables}

fromJSON

\subsection{Learn more webscraping}

Here are some more tips on improving what you can do with webscraping.

\paragraph{Regular Expressions} 

To really become a proficient webscraper it is a good idea to learn POSIX regular expressions\index{regular expressions} and how R implements them.



\paragraph{scraply}  NOT CURRENTLY ON CRAN

\paragraph{Other programming languages}

